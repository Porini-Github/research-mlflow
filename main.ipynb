{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Preamboli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from utils import load_dataset, train_and_log_model\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import MLClient, dsl, command, Input, Output\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.secret import (\n",
    "                             SUBSCRIPTION_ID\n",
    "                            ,RESOURCE_GROUP_NAME\n",
    "                            ,WORKSPACE_NAME\n",
    "                            ,ENVIRONMENT_NAME\n",
    "                            ,ENVIRONMENT_VERSION\n",
    "                            ,COMPUTE_NAME\n",
    "                            ,DATASTORE_NAME\n",
    "                            ,BLOB_ACCOUNT_URL\n",
    "                            ,BLOB_CONTAINER_NAME\n",
    "                            ,MANAGED_IDENTITY_CLIENT_ID\n",
    "                        )\n",
    "\n",
    "DATASTORE_URI = f\"azureml://subscriptions/{SUBSCRIPTION_ID}/resourcegroups/{RESOURCE_GROUP_NAME}/workspaces/{WORKSPACE_NAME}/datastores/{DATASTORE_NAME}/paths/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.config import DEFAULT_INPUT_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 2. Autenticazione e preparazione environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential(\n",
    "    managed_identity_client_id=MANAGED_IDENTITY_CLIENT_ID\n",
    ")\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=SUBSCRIPTION_ID,\n",
    "    resource_group_name=RESOURCE_GROUP_NAME,\n",
    "    workspace_name=WORKSPACE_NAME,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ml_client.environments.get( name = ENVIRONMENT_NAME,\n",
    "                                  version = ENVIRONMENT_VERSION\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 3. Creazione di una Componente per ogni step del ciclo di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_command = command(\n",
    "    name='training_create_data',\n",
    "    command=\"python -m components.01_create_data --input_file_path ${{inputs.input_file_path}} --experiment_name ${{inputs.experiment_name}} --data_asset_name ${{inputs.data_asset_name}} --create_output_path ${{outputs.create_output_path}}\",\n",
    "    environment=env,\n",
    "    code='./',\n",
    "    inputs={'input_file_path': Input(type='string'),\n",
    "            'experiment_name': Input(type='string'), \n",
    "            'data_asset_name': Input(type='string')},\n",
    "    outputs={'create_output_path': Output(type='uri_folder', mode='ro_mount')}\n",
    "    )\n",
    "create_component = ml_client.create_or_update(create_command.component)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 4. Definizione della pipeline (job) ed esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(default_compute=\"cc-azml-01-dev\")\n",
    "def training_pipeline_job(experiment_name, timestamp):\n",
    "    create_job = create_component(input_file_path = DEFAULT_INPUT_FILE_PATH,\n",
    "                                  experiment_name = experiment_name,\n",
    "                                  data_asset_name = 'raw_forecasting_dataset')\n",
    "    # clean_job = clean_component(create_output_path = create_job.outputs.create_output_path,\n",
    "    #                             clean_file_path = DEFAULT_CLEAN_FILE_PATH,\n",
    "    #                             clean_data_asset_name = 'forecasting_dataset',\n",
    "    #                             target_column = 'y',\n",
    "    #                             date_column = 'date')\n",
    "    # train_job = train_component(clean_output_path = clean_job.outputs.clean_output_path,\n",
    "    #                             model_name = 'ForecastModel')\n",
    "    # deploy_job = deploy_component(train_output_path = train_job.outputs.train_output_path,\n",
    "    #                               endpoint_name = DEFAULT_ENDPOINT_NAME,\n",
    "    #                               deployment_name = DEFAULT_DEPLOYMENT_NAME,\n",
    "    #                               environment_name = env_name,\n",
    "    #                               environment_version = str(env_version),\n",
    "    #                               compute = 'cc1-dev03')\n",
    "    # inference_job = inference_component(deploy_output_path = deploy_job.outputs.deploy_output_path,\n",
    "    #                                     input_data_asset_name = 'raw_forecasting_dataset',\n",
    "    #                                     output_data_path = f'batch_output_{timestamp}/{DEFAULT_OUTPUT_DATA_PATH}',\n",
    "    #                                     output_data_asset_name = 'forecasting_prediction')\n",
    "\n",
    "experiment_name = 'training_experiment_0'\n",
    "timestamp = datetime.now(tz=ZoneInfo(\"Europe/Rome\")).strftime(\"%Y%m%d_%H%M%S\")\n",
    "pipeline = training_pipeline_job(experiment_name, timestamp)\n",
    "pipeline_job = ml_client.jobs.create_or_update(pipeline,\n",
    "                                               experiment_name=experiment_name\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# 2. Import e versioning del dataset\n",
    "Viene importato il dataset di esempio (iris oppure breast cancer, in base al parametro settato).\n",
    "\n",
    "Questo dataset viene salvato in locale e versionato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Parametro da settare per scegliere il dataset\n",
    "example_name = \"breast_cancer\" # \"breast_cancer\" / \"iris\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "- *breast cancer dataset:* problema di classificazione binaria (benigno/maligno), 30 features, 569 campioni\n",
    "- *iris dataset:* problema di classificazione multi-classe (setosa/versicolor/virginica), 4 features, 150 campioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = f\"{example_name}_dataset\"\n",
    "\n",
    "X_train, X_test, y_train, y_test, dataset_version, dataset_path, df = load_dataset(dataset_name)\n",
    "\n",
    "print(f\"Dataset salvato in: {dataset_path}\")\n",
    "print(f\"Versione del dataset: {dataset_version}\")\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# 3. Setup MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per impostare l'URI di tracciamento\n",
    "# mlflow.set_tracking_uri(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Lasciando il paramtro di default (in mlflow.set_tracking_uri), i dati vengono salvati in locale nella cartella \"mlruns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impostiamo il nome dell'esperimento\n",
    "mlflow.set_experiment(f\"{example_name}_Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "L'esperimento risulta come una sorta di \"cartella\" sulla UI mlflow per suddividere il tracking di varie runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Runs\n",
    "\n",
    "Vengono effettuate 4 runs diverse per sperimentare 4 diverse combinazioni di parametri nel training del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_randomforest_1 = {\n",
    "    \"model\": \"RandomForest\",\n",
    "    \"n_estimators\": 10, \n",
    "    \"max_depth\": 3,\n",
    "    }\n",
    "\n",
    "D_randomforest_2= {\n",
    "    \"model\": \"RandomForest\",\n",
    "    \"n_estimators\": 20, \n",
    "    \"max_depth\": 5,\n",
    "    }\n",
    "\n",
    "D_logistic_1 = {\n",
    "    \"model\": \"LogisticRegression\",\n",
    "    \"max_iter\": 10\n",
    "    }\n",
    "\n",
    "D_logistic_2 = {\n",
    "    \"model\": \"LogisticRegression\",\n",
    "    \"max_iter\": 15\n",
    "    }\n",
    "\n",
    "D_list = [D_randomforest_1, D_randomforest_2, D_logistic_1, D_logistic_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = f\"{example_name}_Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for D in D_list:\n",
    "    # Eseguiamo alcuni esperimenti con parametri diversi\n",
    "    run_id = train_and_log_model(   model_name = MODEL_NAME,\n",
    "                                    model_dict = D,\n",
    "                                    X_train = X_train,\n",
    "                                    X_test = X_test,\n",
    "                                    y_train = y_train,\n",
    "                                    y_test = y_test,\n",
    "                                    dataset_version = dataset_version,\n",
    "                                    dataset_name = dataset_name,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# 4. Gestione dei modelli e MLflow Model Registry\n",
    "\n",
    "In questo esempio viene ricercato il modello migliore (con un accuracy più alta) tra tutti quelli registrati sotto \"MODEL_NAME\".\n",
    "\n",
    "Successivamente, al modello migliore viene effettuato un cambio di stato, settandolo come modello \"in produzione\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "# 1. Prendi tutte le versioni registrate del modello\n",
    "versions = client.search_model_versions(f\"name='{MODEL_NAME}'\")\n",
    "\n",
    "# 2. Trova l'accuracy migliore tra i run\n",
    "best_run_id = None\n",
    "best_accuracy = -1.0\n",
    "best_model_version = None\n",
    "\n",
    "for v in versions:\n",
    "    run_id = v.run_id\n",
    "    metrics = client.get_run(run_id).data.params\n",
    "    acc = metrics.get(\"accuracy\", None)\n",
    "    if acc is not None and float(acc) > best_accuracy:\n",
    "        best_accuracy = float(acc)\n",
    "        best_run_id = run_id\n",
    "        best_model_version = v.version\n",
    "\n",
    "print(f\"Miglior modello trovato: run_id={best_run_id}, versione={best_model_version}, accuracy={best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Aggiorna lo stato del modello migliore (es. in Production)\n",
    "if best_model_version is not None:\n",
    "    client.transition_model_version_stage(\n",
    "        name=MODEL_NAME,\n",
    "        version=best_model_version,\n",
    "        stage=\"Production\",   # oppure \"Staging\"\n",
    "        archive_existing_versions=True  # sposta gli altri modelli fuori da Production\n",
    "    )\n",
    "    print(f\"Il modello versione {best_model_version} è stato promosso a Production.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# 5. Inference\n",
    "Esempio di inferenza:\n",
    "\n",
    "Utilizzando il modello che è stato messo in produzione, viene fatta inferenza utilizzando 3 nuovi record del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara nuovi dati di input (stesso schema usato in addestramento)\n",
    "if example_name == \"iris\":\n",
    "    # Qui uso l'Iris dataset come esempio\n",
    "    new_data = pd.DataFrame({\n",
    "        \"sepal length (cm)\": [5.1, 6.2, 5.9],\n",
    "        \"sepal width (cm)\":  [3.5, 2.9, 3.0],\n",
    "        \"petal length (cm)\": [1.4, 4.3, 5.1],\n",
    "        \"petal width (cm)\":  [0.2, 1.3, 1.8],\n",
    "    })\n",
    "\n",
    "\n",
    "elif example_name == \"breast_cancer\":\n",
    "    new_data = pd.DataFrame({\n",
    "        \"mean radius\": [14.0, 20.0, 13.5],\n",
    "        \"mean texture\": [20.0, 30.0, 15.0],\n",
    "        \"mean perimeter\": [90.0, 130.0, 85.0],\n",
    "        \"mean area\": [600.0, 1200.0, 500.0],\n",
    "        \"mean smoothness\": [0.1, 0.15, 0.09],\n",
    "        \"mean compactness\": [0.1, 0.2, 0.08],\n",
    "        \"mean concavity\": [0.05, 0.1, 0.04],\n",
    "        \"mean concave points\": [0.02, 0.05, 0.01],\n",
    "        \"mean symmetry\": [0.2, 0.3, 0.18],\n",
    "        \"mean fractal dimension\": [0.06, 0.07, 0.05],\n",
    "        \"radius error\": [0.3, 0.4, 0.2],\n",
    "        \"texture error\": [1.5, 2.0, 1.2],\n",
    "        \"perimeter error\": [2.5, 3.5, 2.0],\n",
    "        \"area error\": [20.0, 30.0, 15.0],\n",
    "        \"smoothness error\": [0.005, 0.007, 0.004],\n",
    "        \"compactness error\": [0.02, 0.03, 0.015],\n",
    "        \"concavity error\": [0.02, 0.025, 0.01],\n",
    "        \"concave points error\": [0.01, 0.015, 0.008],\n",
    "        \"symmetry error\": [0.02, 0.03, 0.015],\n",
    "        \"fractal dimension error\": [0.003, 0.004, 0.002],\n",
    "        \"worst radius\": [16.0, 25.0, 14.5],\n",
    "        \"worst texture\": [25.0, 40.0, 20.0],\n",
    "        \"worst perimeter\": [110.0, 160.0, 95.0],\n",
    "        \"worst area\": [800.0, 1500.0, 600.0],\n",
    "        \"worst smoothness\": [0.15, 0.2, 0.12],\n",
    "        \"worst compactness\": [0.25, 0.3, 0.2],\n",
    "        \"worst concavity\": [0.15, 0.2, 0.1],\n",
    "        \"worst concave points\": [0.07, 0.1, 0.05],\n",
    "        \"worst symmetry\": [0.3, 0.4, 0.25],\n",
    "        \"worst fractal dimension\": [0.08, 0.1, 0.07],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera il modello in produzione\n",
    "model_uri = f\"models:/{MODEL_NAME}/Production\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "\n",
    "# Esegui inferenza\n",
    "predictions = model.predict(new_data)\n",
    "\n",
    "print(\"Inferenza:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Per fare pratica:\n",
    "- Utilizzare un dataset differente, un dataset custom o un altro pubblico tipo \"wine quality\", basta che sia un problema di classificazione. Eventualmente, serve modificare il parametro \"example_name\", la funzione \"load_dataset\" e modificare i \"new_data\" per la parte di inferenza.\n",
    "- Sperimentare più combinazioni di parametri/modelli.\n",
    "- Aggiungere metriche di valutazione del modello (e loggarle su mlflow).\n",
    "- Parametrizzare la logica di split train/test (e loggarla come parametro su mlflow).\n",
    "- Cambiare di stato i modelli nel model registry sulla base di logiche a propria scelta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
