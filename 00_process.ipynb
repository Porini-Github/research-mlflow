{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Preamboli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import pickle, tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_version(dataset_name: str, df: pd.DataFrame):\n",
    "    \"\"\"Salva il dataframe in una cartella \"datasets/<dataset_name>/\"\n",
    "    con un nome di file che segue lo schema vXX.csv, dove XX è un numero\n",
    "    incrementale che rappresenta la versione del dataset\"\"\"\n",
    "    \n",
    "    # Percorso della cartella dataset\n",
    "    base_dir = \"datasets\"\n",
    "    dataset_dir = os.path.join(base_dir, dataset_name)\n",
    "    \n",
    "    # Crea la cartella se non esiste\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    \n",
    "    # Trova i file già presenti che matchano lo schema vXX.csv\n",
    "    existing_files = [f for f in os.listdir(dataset_dir) if re.match(r\"v\\d{2}\\.csv\", f)]\n",
    "    \n",
    "    if not existing_files:\n",
    "        # Se non ci sono file, la prima versione è v01\n",
    "        version_number = 1\n",
    "    else:\n",
    "        # Estrai i numeri delle versioni dai file\n",
    "        versions = [int(re.findall(r\"\\d{2}\", f)[0]) for f in existing_files]\n",
    "        version_number = max(versions) + 1\n",
    "    \n",
    "    # Nome del file da salvare\n",
    "    filename = f\"v{version_number:02d}.csv\"\n",
    "    filepath = os.path.join(dataset_dir, filename)\n",
    "    \n",
    "    # Salva il dataframe\n",
    "    df.to_csv(filepath, index=False)\n",
    "    \n",
    "    return filepath, version_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    Carica un dataset in base al nome fornito.\n",
    "    Supporta \"iris_dataset\" e \"breast_cancer_dataset\".\n",
    "    Restituisce X_train, X_test, y_train, y_test, dataset_version, dataset_path, df\n",
    "    \"\"\"\n",
    "    if dataset_name == \"iris_dataset\":\n",
    "        # Carichiamo il dataset\n",
    "        iris = load_iris()\n",
    "        df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "        df['target'] = iris.target\n",
    "\n",
    "        # Salviamo il dataset versionato localmente (puoi anche usare DVC o Git LFS)\n",
    "        dataset_path, dataset_version = save_dataset_version(dataset_name, df)\n",
    "\n",
    "        # Split train/test\n",
    "        X = df[iris.feature_names]\n",
    "        y = df['target']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "    elif dataset_name == \"breast_cancer_dataset\":\n",
    "        # Carichiamo il dataset Breast Cancer\n",
    "        cancer = load_breast_cancer()\n",
    "        df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "        df['target'] = cancer.target\n",
    "\n",
    "        # Salviamo il dataset versionato localmente\n",
    "        dataset_path, dataset_version = save_dataset_version(dataset_name, df)\n",
    "\n",
    "        # Split train/test\n",
    "        X = df[cancer.feature_names]\n",
    "        y = df['target']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.3, random_state=42\n",
    "        )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, dataset_version, dataset_path, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione helper per loggare esperimenti\n",
    "def train_and_log_model(model_name=\"IrisClassifier\",\n",
    "                        model_dict = {\n",
    "                            \"model\": \"RandomForest\",\n",
    "                            \"n_estimators\": 100,\n",
    "                            \"max_depth\": None\n",
    "                        },\n",
    "                        X_train=None,\n",
    "                        X_test=None,\n",
    "                        y_train=None,\n",
    "                        y_test=None,\n",
    "                        dataset_version=None,\n",
    "                        dataset_name=None,\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Funzione per addestrare un modello, calcolare metriche e loggare tutto su MLflow.\n",
    "    model_dict: dizionario con i parametri del modello\n",
    "    model_name: nome del modello registrato in MLflow\n",
    "    X_train, X_test, y_train, y_test: dati di addestramento e test\n",
    "    dataset_version: versione del dataset (se disponibile)\n",
    "    dataset_name: nome del dataset (se disponibile)\n",
    "    \"\"\"\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        if model_dict[\"model\"] == \"RandomForest\":\n",
    "            # 1. Crea il modello\n",
    "            model = RandomForestClassifier(n_estimators=model_dict[\"n_estimators\"], max_depth=model_dict[\"max_depth\"], random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            mlflow.log_param(\"n_estimators\", model_dict[\"n_estimators\"])\n",
    "            mlflow.log_param(\"max_depth\", model_dict[\"max_depth\"])\n",
    "\n",
    "            # Log feature importance (se utile per analisi)\n",
    "            feature_importances = dict(zip(X_train.columns, model.feature_importances_))\n",
    "            mlflow.log_dict(feature_importances, \"feature_importances.json\")\n",
    "\n",
    "        elif model_dict[\"model\"] == \"LogisticRegression\":\n",
    "            model = LogisticRegression(max_iter=model_dict[\"max_iter\"], random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            mlflow.log_param(\"max_iter\", model_dict[\"max_iter\"])\n",
    "\n",
    "\n",
    "        # 2. Previsioni e metriche\n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, preds)\n",
    "        prec = precision_score(y_test, preds, average=\"weighted\")\n",
    "        rec = recall_score(y_test, preds, average=\"weighted\")\n",
    "        f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "\n",
    "        # 3. Log parametri principali del modello\n",
    "        mlflow.log_param(\"model_class\", model_dict[\"model\"])\n",
    "        if dataset_version is not None:\n",
    "            mlflow.log_param(\"dataset_version\", dataset_version)\n",
    "        if dataset_name is not None:\n",
    "            mlflow.log_param(\"dataset_name\", dataset_name)\n",
    "\n",
    "\n",
    "        # 4. Log metriche\n",
    "        mlflow.log_param(\"accuracy\", round( float(acc), 2))\n",
    "        mlflow.log_param(\"precision_weighted\", round( float(prec), 2))\n",
    "        mlflow.log_param(\"recall_weighted\", round( float(rec), 2))\n",
    "        mlflow.log_param(\"f1_weighted\", round( float(f1), 2))\n",
    "\n",
    "        # ---  Salvataggio locale + upload negli artifacts\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            model_path = os.path.join(tmpdir, f\"{model_name}.pkl\")\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                pickle.dump(model, f)\n",
    "\n",
    "            # Scrivi info ambiente\n",
    "            env_info = {\n",
    "                \"python_version\": sys.version,\n",
    "                \"platform\": platform.platform(),\n",
    "                \"mlflow_version\": mlflow.__version__,\n",
    "                \"sklearn_version\": model.__module__.split('.')[0]\n",
    "            }\n",
    "            env_path = os.path.join(tmpdir, \"environment_info.json\")\n",
    "            with open(env_path, \"w\") as f:\n",
    "                json.dump(env_info, f, indent=2)\n",
    "\n",
    "            # Scrivi requirements standardizzati\n",
    "            reqs_path = os.path.join(tmpdir, \"requirements.txt\")\n",
    "            with open(reqs_path, \"w\") as f:\n",
    "                f.write(\"scikit-learn\\nmlflow\\npandas\\nnumpy\\n\")\n",
    "\n",
    "            # Log degli artifacts (modello + env)\n",
    "            mlflow.log_artifacts(tmpdir, artifact_path=\"model_package\")\n",
    "\n",
    "        # 5. Log del modello e dell’ambiente\n",
    "        mlflow.sklearn.log_model(model, \"model\", registered_model_name=model_name)\n",
    "\n",
    "        conda_env = mlflow.sklearn.get_default_conda_env()\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            conda_env=conda_env\n",
    "        )\n",
    "\n",
    "        # N.B. Non per forza tutti i modelli vanno registrati. \n",
    "        # Volendo, si può decidere di non registrare i modelli dentro questo codice, ma decidere a posteriori quali registrare e quali no\n",
    "\n",
    "        # 6. Log di due righe di esempio dal dataset\n",
    "        sample_input = X_train.head(2).copy()\n",
    "        sample_input[\"target\"] = y_train.iloc[:2].values\n",
    "        mlflow.log_table(sample_input, \"sample_input.parquet\")\n",
    "\n",
    "        # 7. Stampa riassunto\n",
    "        print(f\"Run {run.info.run_id} - Acc: {acc:.4f}\")\n",
    "        \n",
    "        return run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# 2. Import e versioning del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_name = \"breast_cancer\" # \"breast_cancer\" / \"iris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = f\"{example_name}_dataset\" #  \"breast_cancer_dataset\" / \"iris_dataset\"\n",
    "\n",
    "X_train, X_test, y_train, y_test, dataset_version, dataset_path, df = load_dataset(dataset_name)\n",
    "\n",
    "print(f\"Dataset salvato in: {dataset_path}\")\n",
    "print(f\"Versione del dataset: {dataset_version}\")\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 3. Setup MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per impostare l'URI di tracciamento \n",
    "# mlflow.set_tracking_uri(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impostiamo il nome dell'esperimento\n",
    "mlflow.set_experiment(f\"{example_name}_Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_randomforest_1 = {\n",
    "    \"model\": \"RandomForest\",\n",
    "    \"n_estimators\": 10, \n",
    "    \"max_depth\": 3,\n",
    "    }\n",
    "\n",
    "D_randomforest_2= {\n",
    "    \"model\": \"RandomForest\",\n",
    "    \"n_estimators\": 20, \n",
    "    \"max_depth\": 5,\n",
    "    }\n",
    "\n",
    "D_logistic_1 = {\n",
    "    \"model\": \"LogisticRegression\",\n",
    "    \"max_iter\": 10\n",
    "    }\n",
    "\n",
    "\n",
    "D_logistic_2 = {\n",
    "    \"model\": \"LogisticRegression\",\n",
    "    \"max_iter\": 10\n",
    "    }\n",
    "\n",
    "D_list = [D_randomforest_1, D_randomforest_2, D_logistic_1, D_logistic_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = f\"{example_name}_Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for D in D_list:\n",
    "    # Eseguiamo alcuni esperimenti con parametri diversi\n",
    "    run_id = train_and_log_model(   model_name = MODEL_NAME,\n",
    "                                    model_dict = D,\n",
    "                                    X_train = X_train,\n",
    "                                    X_test = X_test,\n",
    "                                    y_train = y_train,\n",
    "                                    y_test = y_test,\n",
    "                                    dataset_version = dataset_version,\n",
    "                                    dataset_name = dataset_name,\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# 4. Gestione dei modelli e MLflow Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "\n",
    "# 1. Prendi tutte le versioni registrate del modello\n",
    "versions = client.search_model_versions(f\"name='{MODEL_NAME}'\")\n",
    "\n",
    "# 2. Trova l'accuracy migliore tra i run\n",
    "best_run_id = None\n",
    "best_accuracy = -1.0\n",
    "best_model_version = None\n",
    "\n",
    "for v in versions:\n",
    "    run_id = v.run_id\n",
    "    metrics = client.get_run(run_id).data.params\n",
    "    acc = metrics.get(\"accuracy\", None)\n",
    "    if acc is not None and float(acc) > best_accuracy:\n",
    "        best_accuracy = float(acc)\n",
    "        best_run_id = run_id\n",
    "        best_model_version = v.version\n",
    "\n",
    "print(f\"Miglior modello trovato: run_id={best_run_id}, versione={best_model_version}, accuracy={best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Aggiorna lo stato del modello migliore (es. in Production)\n",
    "if best_model_version is not None:\n",
    "    client.transition_model_version_stage(\n",
    "        name=MODEL_NAME,\n",
    "        version=best_model_version,\n",
    "        stage=\"Production\",   # oppure \"Staging\"\n",
    "        archive_existing_versions=True  # sposta gli altri modelli fuori da Production\n",
    "    )\n",
    "    print(f\"Il modello versione {best_model_version} è stato promosso a Production.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera il modello in produzione\n",
    "model_uri = f\"models:/{MODEL_NAME}/Production\"\n",
    "model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara nuovi dati di input (stesso schema usato in addestramento)\n",
    "if example_name == \"iris\":\n",
    "    # Qui uso l'Iris dataset come esempio\n",
    "    new_data = pd.DataFrame({\n",
    "        \"sepal length (cm)\": [5.1, 6.2, 5.9],\n",
    "        \"sepal width (cm)\":  [3.5, 2.9, 3.0],\n",
    "        \"petal length (cm)\": [1.4, 4.3, 5.1],\n",
    "        \"petal width (cm)\":  [0.2, 1.3, 1.8],\n",
    "    })\n",
    "\n",
    "\n",
    "elif example_name == \"breast_cancer\":\n",
    "    new_data = pd.DataFrame({\n",
    "        \"mean radius\": [14.0, 20.0, 13.5],\n",
    "        \"mean texture\": [20.0, 30.0, 15.0],\n",
    "        \"mean perimeter\": [90.0, 130.0, 85.0],\n",
    "        \"mean area\": [600.0, 1200.0, 500.0],\n",
    "        \"mean smoothness\": [0.1, 0.15, 0.09],\n",
    "        \"mean compactness\": [0.1, 0.2, 0.08],\n",
    "        \"mean concavity\": [0.05, 0.1, 0.04],\n",
    "        \"mean concave points\": [0.02, 0.05, 0.01],\n",
    "        \"mean symmetry\": [0.2, 0.3, 0.18],\n",
    "        \"mean fractal dimension\": [0.06, 0.07, 0.05],\n",
    "        \"radius error\": [0.3, 0.4, 0.2],\n",
    "        \"texture error\": [1.5, 2.0, 1.2],\n",
    "        \"perimeter error\": [2.5, 3.5, 2.0],\n",
    "        \"area error\": [20.0, 30.0, 15.0],\n",
    "        \"smoothness error\": [0.005, 0.007, 0.004],\n",
    "        \"compactness error\": [0.02, 0.03, 0.015],\n",
    "        \"concavity error\": [0.02, 0.025, 0.01],\n",
    "        \"concave points error\": [0.01, 0.015, 0.008],\n",
    "        \"symmetry error\": [0.02, 0.03, 0.015],\n",
    "        \"fractal dimension error\": [0.003, 0.004, 0.002],\n",
    "        \"worst radius\": [16.0, 25.0, 14.5],\n",
    "        \"worst texture\": [25.0, 40.0, 20.0],\n",
    "        \"worst perimeter\": [110.0, 160.0, 95.0],\n",
    "        \"worst area\": [800.0, 1500.0, 600.0],\n",
    "        \"worst smoothness\": [0.15, 0.2, 0.12],\n",
    "        \"worst compactness\": [0.25, 0.3, 0.2],\n",
    "        \"worst concavity\": [0.15, 0.2, 0.1],\n",
    "        \"worst concave points\": [0.07, 0.1, 0.05],\n",
    "        \"worst symmetry\": [0.3, 0.4, 0.25],\n",
    "        \"worst fractal dimension\": [0.08, 0.1, 0.07],\n",
    "    })\n",
    "\n",
    "\n",
    "# 3. Esegui inferenza\n",
    "predictions = model.predict(new_data)\n",
    "\n",
    "print(\"Inferenza:\")\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
